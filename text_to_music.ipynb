{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKWawKUQt2xTEmWUQNQk3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshitkt001/Music_GenAI/blob/main/text_to_music.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFk9dBuj3bel",
        "outputId": "7d115a37-6f84-47c9-ea0b-7af34aa65661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "id": "IaAgvJuk3fXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5516c7-39cc-49ff-be0c-0a4479cbc1cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Hyperparameters\n",
        "text_sequence_length = 50  # Length of input text sequences\n",
        "music_sequence_length = 100  # Length of music sequences\n",
        "embedding_dim = 128  # Dimension of word embeddings\n",
        "lstm_units = 256  # Number of LSTM units\n",
        "output_dim = 128  # Number of units in the Dense output layer\n",
        "\n",
        "# Define vocabulary size and music feature dimensions\n",
        "vocab_size = 10000  # Replace this with the actual size of your vocabulary\n",
        "music_feature_dim = 128  # Replace this with the actual dimension of your music features\n",
        "\n",
        "# Define input layers\n",
        "text_input = Input(shape=(text_sequence_length,), name='text_input')\n",
        "music_features_input = Input(shape=(music_sequence_length, music_feature_dim), name='music_features_input')\n",
        "\n",
        "# Text embedding layer\n",
        "text_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
        "\n",
        "# LSTM layers\n",
        "lstm_layer1 = LSTM(lstm_units, return_sequences=True)(text_embedding)\n",
        "lstm_layer2 = LSTM(lstm_units, return_sequences=True)(lstm_layer1)\n",
        "lstm_layer3 = LSTM(lstm_units, return_sequences=True)(lstm_layer2)\n",
        "lstm_layer4 = LSTM(lstm_units)(lstm_layer3)  # Last layer without return sequences\n",
        "\n",
        "# RepeatVector to match time steps for music generation\n",
        "repeat_layer = RepeatVector(music_sequence_length)(lstm_layer4)\n",
        "\n",
        "# Concatenate text features with music features\n",
        "concatenated = Concatenate(axis=-1)([repeat_layer, music_features_input])\n",
        "\n",
        "# LSTM layer for music generation\n",
        "music_lstm = LSTM(lstm_units, return_sequences=True)(concatenated)\n",
        "\n",
        "# Output layer for music generation\n",
        "music_output = Dense(output_dim)(music_lstm)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[text_input, music_features_input], outputs=music_output)\n",
        "\n",
        "# Compile the model (specify optimizer and loss function)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "MSFug4Dz3fgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b562790-3007-4630-add5-aa4992f6c8d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text_input (InputLayer)        [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 50, 128)      1280000     ['text_input[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_9 (LSTM)                  (None, 50, 256)      394240      ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_10 (LSTM)                 (None, 50, 256)      525312      ['lstm_9[0][0]']                 \n",
            "                                                                                                  \n",
            " lstm_11 (LSTM)                 (None, 50, 256)      525312      ['lstm_10[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_12 (LSTM)                 (None, 256)          525312      ['lstm_11[0][0]']                \n",
            "                                                                                                  \n",
            " repeat_vector_1 (RepeatVector)  (None, 100, 256)    0           ['lstm_12[0][0]']                \n",
            "                                                                                                  \n",
            " music_features_input (InputLay  [(None, 100, 128)]  0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 100, 384)     0           ['repeat_vector_1[0][0]',        \n",
            "                                                                  'music_features_input[0][0]']   \n",
            "                                                                                                  \n",
            " lstm_13 (LSTM)                 (None, 100, 256)     656384      ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100, 128)     32896       ['lstm_13[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,939,456\n",
            "Trainable params: 3,939,456\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "text_sequence_length = 50\n",
        "music_sequence_length = 100\n",
        "embedding_dim = 128\n",
        "lstm_units = 256\n",
        "output_dim = 128\n",
        "vocab_size = 10000\n",
        "music_feature_dim = 128\n",
        "\n",
        "# Define input layers\n",
        "text_input = Input(shape=(text_sequence_length,), name='text_input')\n",
        "music_features_input = Input(shape=(music_sequence_length, music_feature_dim), name='music_features_input')\n",
        "\n",
        "# Text embedding layer\n",
        "text_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
        "\n",
        "# LSTM layers for text analysis\n",
        "lstm_layer1 = LSTM(lstm_units, return_sequences=True)(text_embedding)\n",
        "lstm_layer2 = LSTM(lstm_units, return_sequences=True)(lstm_layer1)\n",
        "lstm_layer3 = LSTM(lstm_units, return_sequences=True)(lstm_layer2)\n",
        "lstm_layer4 = LSTM(lstm_units)(lstm_layer3)\n",
        "\n",
        "# RepeatVector to match time steps for music generation\n",
        "repeat_layer = RepeatVector(music_sequence_length)(lstm_layer4)\n",
        "\n",
        "# Concatenate text features with music features\n",
        "concatenated = Concatenate(axis=-1)([repeat_layer, music_features_input])\n",
        "\n",
        "# LSTM layer for music generation\n",
        "music_lstm = LSTM(lstm_units, return_sequences=True)(concatenated)\n",
        "\n",
        "# Output layer for music generation\n",
        "music_output = Dense(output_dim)(music_lstm)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[text_input, music_features_input], outputs=music_output)\n",
        "\n",
        "# Compile the model (specify optimizer and loss function)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Generate some dummy data for demonstration\n",
        "num_samples = 1000\n",
        "dummy_text_data = np.random.randint(0, vocab_size, size=(num_samples, text_sequence_length))\n",
        "dummy_music_features = np.random.random((num_samples, music_sequence_length, music_feature_dim))\n",
        "dummy_music_labels = np.random.random((num_samples, music_sequence_length, output_dim))\n",
        "\n",
        "# Train the model\n",
        "model.fit(x=[dummy_text_data, dummy_music_features], y=dummy_music_labels, epochs=10)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('text_to_music_model.h5')\n",
        "\n",
        "# Load the trained model\n",
        "loaded_model = tf.keras.models.load_model('text_to_music_model.h5')\n",
        "\n",
        "# Now you can use the loaded model for text-to-music generation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJlwW0o3e_J9",
        "outputId": "919e33a8-318e-43c4-8247-63b67e3aa0fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text_input (InputLayer)        [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 50, 128)      1280000     ['text_input[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_14 (LSTM)                 (None, 50, 256)      394240      ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_15 (LSTM)                 (None, 50, 256)      525312      ['lstm_14[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_16 (LSTM)                 (None, 50, 256)      525312      ['lstm_15[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_17 (LSTM)                 (None, 256)          525312      ['lstm_16[0][0]']                \n",
            "                                                                                                  \n",
            " repeat_vector_2 (RepeatVector)  (None, 100, 256)    0           ['lstm_17[0][0]']                \n",
            "                                                                                                  \n",
            " music_features_input (InputLay  [(None, 100, 128)]  0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 100, 384)     0           ['repeat_vector_2[0][0]',        \n",
            "                                                                  'music_features_input[0][0]']   \n",
            "                                                                                                  \n",
            " lstm_18 (LSTM)                 (None, 100, 256)     656384      ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 100, 128)     32896       ['lstm_18[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,939,456\n",
            "Trainable params: 3,939,456\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 70s 2s/step - loss: 0.1106\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.0842\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 58s 2s/step - loss: 0.0838\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.0837\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.0836\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.0836\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 58s 2s/step - loss: 0.0836\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 56s 2s/step - loss: 0.0835\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.0835\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 57s 2s/step - loss: 0.0835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define your preprocess_text function\n",
        "def preprocess_text(input_text, tokenizer, max_seq_length):\n",
        "    # Tokenize the input text\n",
        "    tokenized_text = tokenizer.texts_to_sequences([input_text])\n",
        "\n",
        "    # Pad the tokenized sequence\n",
        "    padded_text = pad_sequences(tokenized_text, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "    return padded_text\n",
        "\n",
        "# Load the trained model\n",
        "loaded_model = tf.keras.models.load_model('text_to_music_model.h5')\n",
        "\n",
        "# Load or create a tokenizer based on your training data\n",
        "# tokenizer = ...  # Load your tokenizer\n",
        "max_seq_length = 50  # Maximum sequence length used in training\n",
        "\n",
        "# Get user input\n",
        "user_input = input(\"Enter a text: \")\n",
        "\n",
        "# Preprocess the text input using the defined function\n",
        "processed_text = preprocess_text(user_input, tokenizer, max_seq_length)\n",
        "\n",
        "# Generate music features using the loaded model\n",
        "dummy_music_features = np.random.random((1, music_sequence_length, music_feature_dim))  # You might replace this with real music features\n",
        "\n",
        "generated_music = loaded_model.predict([processed_text, dummy_music_features])\n",
        "\n",
        "# Print or visualize the generated music features\n",
        "print(generated_music)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "b1eg5CWlhY4w",
        "outputId": "54e69eb8-e9f5-4416-f0b4-56136632cd95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a text: love is in the air\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-352c711b8ef2>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Preprocess the text input using the defined function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Generate music features using the loaded model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample texts for demonstration\n",
        "train_texts = [\n",
        "    \"Once upon a time in a land far away...\",\n",
        "    \"Roses are red, violets are blue...\",\n",
        "    \"It was the best of times, it was the worst of times...\",\n",
        "    \"To be or not to be, that is the question...\",\n",
        "    # Add more sample texts here\n",
        "]\n",
        "\n",
        "# Assuming you have corresponding music features for each text\n",
        "# Create dummy music features for demonstration\n",
        "num_samples = len(train_texts)\n",
        "music_sequence_length = 100\n",
        "music_feature_dim = 128\n",
        "train_music_features = np.random.random((num_samples, music_sequence_length, music_feature_dim))\n",
        "\n",
        "# Rest of your code...\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Generate dummy data for demonstration\n",
        "num_samples = 1000\n",
        "train_texts = [f\"sample text {i}\" for i in range(num_samples)]\n",
        "train_music_features = np.random.random((num_samples, music_sequence_length, music_feature_dim))\n",
        "train_music_labels = np.random.random((num_samples, music_sequence_length, output_dim))\n",
        "\n",
        "# Load or create a tokenizer based on your training data\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Define your preprocess_text function\n",
        "def preprocess_text(input_text, tokenizer, max_seq_length):\n",
        "    tokenized_text = tokenizer.texts_to_sequences([input_text])\n",
        "    padded_text = pad_sequences(tokenized_text, maxlen=max_seq_length, padding='post')\n",
        "    return padded_text\n",
        "\n",
        "# Load the trained model\n",
        "loaded_model = tf.keras.models.load_model('text_to_music_model.h5')\n",
        "\n",
        "# Maximum sequence length used in training\n",
        "max_seq_length = 50\n",
        "\n",
        "# Get user input\n",
        "user_input = input(\"Enter a text: \")\n",
        "\n",
        "# Preprocess the text input using the defined function\n",
        "processed_text = preprocess_text(user_input, tokenizer, max_seq_length)\n",
        "\n",
        "# Generate music features using the loaded model\n",
        "dummy_music_features = np.random.random((1, music_sequence_length, music_feature_dim))\n",
        "\n",
        "generated_music = loaded_model.predict([processed_text, dummy_music_features])\n",
        "\n",
        "# Print or visualize the generated music features\n",
        "print(generated_music)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s245CbVVjnY7",
        "outputId": "4a2b2c97-0cba-44c3-9c8a-73bf7a72a3fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text: Roses are red, violets are blue you are beautiful just like you.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "[[[0.43315443 0.43535122 0.5254298  ... 0.42742342 0.48194125 0.41093925]\n",
            "  [0.49938145 0.48479247 0.50733525 ... 0.47366348 0.4932505  0.4884351 ]\n",
            "  [0.5022191  0.49300835 0.50166124 ... 0.495313   0.4740264  0.5034019 ]\n",
            "  ...\n",
            "  [0.5000275  0.49020535 0.4987788  ... 0.49359336 0.49113816 0.51012945]\n",
            "  [0.49886018 0.49719608 0.5035875  ... 0.4929995  0.47955528 0.50978965]\n",
            "  [0.49083853 0.4962536  0.51700926 ... 0.48702016 0.4895042  0.5014919 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30m5C7AikRMV",
        "outputId": "9d8e3401-6782-4f16-bdbd-1d1798d63a3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# Assuming `generated_music` contains the generated music features\n",
        "\n",
        "# Scale the generated music features to the desired audio range\n",
        "scaled_music = (generated_music - generated_music.min()) / (generated_music.max() - generated_music.min())\n",
        "scaled_music = scaled_music * 2 - 1  # Map to the range [-1, 1]\n",
        "\n",
        "# Convert scaled music to audio\n",
        "audio_data = scaled_music.flatten()\n",
        "audio_data = (audio_data * 32767).astype(np.int16)  # Convert to 16-bit PCM format\n",
        "\n",
        "# Create an AudioSegment object\n",
        "audio_segment = AudioSegment(\n",
        "    audio_data.tobytes(),\n",
        "    frame_rate=44100,  # Adjust the frame rate as needed\n",
        "    sample_width=2,    # 16-bit audio\n",
        "    channels=1          # Mono audio\n",
        ")\n",
        "\n",
        "# Save the audio segment as a WAV file\n",
        "audio_filename = 'generated_music.wav'\n",
        "audio_segment.export(audio_filename, format='wav')\n",
        "\n",
        "print(f\"Generated music saved as {audio_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f2NvP6ZkV-r",
        "outputId": "68b180b9-f009-46fb-befa-067e28b453bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated music saved as generated_music.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Assuming `loaded_model` and `preprocess_text` function are defined\n",
        "\n",
        "# User input\n",
        "user_input = input(\"Enter a text: \")\n",
        "\n",
        "# Preprocess the text input\n",
        "processed_text = preprocess_text(user_input, tokenizer, max_seq_length)\n",
        "\n",
        "# Generate music features using the loaded model\n",
        "dummy_music_features = np.random.random((1, music_sequence_length, music_feature_dim))  # Replace with real music features\n",
        "\n",
        "# Generate multiple segments of music\n",
        "num_segments = 5  # Number of segments to generate\n",
        "generated_segments = []\n",
        "\n",
        "for _ in range(num_segments):\n",
        "    generated_music = loaded_model.predict([processed_text, dummy_music_features])\n",
        "    generated_segments.append(generated_music)\n",
        "\n",
        "# Rest of your code...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "9vOHpkhLHCsL",
        "outputId": "9773a910-5ed6-4b44-e0b7-93c2dd3acd41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d91a320c3922>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Assuming `loaded_model` and `preprocess_text` function are defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Hyperparameters\n",
        "text_sequence_length = 50\n",
        "music_sequence_length = 100\n",
        "embedding_dim = 128\n",
        "lstm_units = 256\n",
        "output_dim = 128\n",
        "vocab_size = 10000\n",
        "music_feature_dim = 128\n",
        "\n",
        "# Define input layers\n",
        "text_input = Input(shape=(text_sequence_length,), name='text_input')\n",
        "music_features_input = Input(shape=(music_sequence_length, music_feature_dim), name='music_features_input')\n",
        "\n",
        "# Text embedding layer\n",
        "text_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
        "\n",
        "# LSTM layers for text analysis\n",
        "lstm_layer1 = LSTM(lstm_units, return_sequences=True)(text_embedding)\n",
        "lstm_layer2 = LSTM(lstm_units, return_sequences=True)(lstm_layer1)\n",
        "lstm_layer3 = LSTM(lstm_units, return_sequences=True)(lstm_layer2)\n",
        "lstm_layer4 = LSTM(lstm_units)(lstm_layer3)\n",
        "\n",
        "# RepeatVector to match time steps for music generation\n",
        "repeat_layer = RepeatVector(music_sequence_length)(lstm_layer4)\n",
        "\n",
        "# Concatenate text features with music features\n",
        "concatenated = Concatenate(axis=-1)([repeat_layer, music_features_input])\n",
        "\n",
        "# LSTM layer for music generation\n",
        "music_lstm = LSTM(lstm_units, return_sequences=True)(concatenated)\n",
        "\n",
        "# Output layer for music generation\n",
        "music_output = Dense(output_dim)(music_lstm)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[text_input, music_features_input], outputs=music_output)\n",
        "\n",
        "# Compile the model (specify optimizer and loss function)\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Generate some dummy data for demonstration\n",
        "num_samples = 1000\n",
        "dummy_text_data = np.random.randint(0, vocab_size, size=(num_samples, text_sequence_length))\n",
        "dummy_music_features = np.random.random((num_samples, music_sequence_length, music_feature_dim))\n",
        "dummy_music_labels = np.random.random((num_samples, music_sequence_length, output_dim))\n",
        "\n",
        "# Train the model\n",
        "model.fit(x=[dummy_text_data, dummy_music_features], y=dummy_music_labels, epochs=10)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('text_to_music_model.h5')\n",
        "\n",
        "# Load the trained model\n",
        "loaded_model = tf.keras.models.load_model('text_to_music_model.h5')\n",
        "\n",
        "# Sample texts for demonstration\n",
        "train_texts = [\n",
        "    \"Once upon a time in a land far away...\",\n",
        "    \"Roses are red, violets are blue...\",\n",
        "    \"It was the best of times, it was the worst of times...\",\n",
        "    \"To be or not to be, that is the question...\",\n",
        "    # Add more sample texts here\n",
        "]\n",
        "\n",
        "# Assuming you have corresponding music features for each text\n",
        "# Create dummy music features for demonstration\n",
        "num_samples = len(train_texts)\n",
        "music_sequence_length = 100\n",
        "music_feature_dim = 128\n",
        "train_music_features = np.random.random((num_samples, music_sequence_length, music_feature_dim))\n",
        "\n",
        "# User input\n",
        "user_input = input(\"Enter a text: \")\n",
        "\n",
        "# Preprocess the text input\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "max_seq_length = text_sequence_length\n",
        "processed_text = tokenizer.texts_to_sequences([user_input])\n",
        "processed_text = tf.keras.preprocessing.sequence.pad_sequences(processed_text, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Generate music features using the loaded model\n",
        "dummy_music_features = np.random.random((1, music_sequence_length, music_feature_dim))  # Replace with real music features\n",
        "\n",
        "# Generate multiple segments of music\n",
        "num_segments = 5  # Number of segments to generate\n",
        "generated_segments = []\n",
        "\n",
        "for _ in range(num_segments):\n",
        "    generated_music = loaded_model.predict([processed_text, dummy_music_features])\n",
        "    generated_segments.append(generated_music)\n",
        "\n",
        "# Combine the generated music segments into a single audio segment\n",
        "combined_audio_segment = AudioSegment.silent(duration=0)  # Initialize an empty audio segment\n",
        "\n",
        "# Convert and add each generated music segment\n",
        "for segment in generated_segments:\n",
        "    scaled_music = (segment - segment.min()) / (segment.max() - segment.min())\n",
        "    scaled_music = scaled_music * 2 - 1\n",
        "    audio_data = scaled_music.flatten()\n",
        "    audio_data = (audio_data * 32767).astype(np.int16)\n",
        "\n",
        "    audio_segment = AudioSegment(\n",
        "        audio_data.tobytes(),\n",
        "        frame_rate=44100,  # Adjust the frame rate as needed\n",
        "        sample_width=2,    # 16-bit audio\n",
        "        channels=1          # Mono audio\n",
        "    )\n",
        "\n",
        "    combined_audio_segment += audio_segment\n",
        "\n",
        "# Export the combined audio segment as a WAV file\n",
        "combined_audio_filename = 'combined_music.wav'\n",
        "combined_audio_segment.export(combined_audio_filename, format='wav')\n",
        "\n",
        "print(f\"Combined music saved as {combined_audio_filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2reidNPJ8iN",
        "outputId": "2fa00479-4bd2-4a74-f903-109c44354e5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " text_input (InputLayer)        [(None, 50)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 50, 128)      1280000     ['text_input[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 50, 256)      394240      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 50, 256)      525312      ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  (None, 50, 256)      525312      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  (None, 256)          525312      ['lstm_2[0][0]']                 \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVector)   (None, 100, 256)     0           ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " music_features_input (InputLay  [(None, 100, 128)]  0           []                               \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 100, 384)     0           ['repeat_vector[0][0]',          \n",
            "                                                                  'music_features_input[0][0]']   \n",
            "                                                                                                  \n",
            " lstm_4 (LSTM)                  (None, 100, 256)     656384      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 100, 128)     32896       ['lstm_4[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,939,456\n",
            "Trainable params: 3,939,456\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 69s 2s/step - loss: 0.1077\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 56s 2s/step - loss: 0.0842\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 55s 2s/step - loss: 0.0838\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 56s 2s/step - loss: 0.0837\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 56s 2s/step - loss: 0.0836\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 55s 2s/step - loss: 0.0836\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 56s 2s/step - loss: 0.0835\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 55s 2s/step - loss: 0.0835\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 55s 2s/step - loss: 0.0835\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 55s 2s/step - loss: 0.0835\n",
            "Enter a text: roses are red, sky is blue i am holding your hand and feeling you.\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "Combined music saved as combined_music.wav\n"
          ]
        }
      ]
    }
  ]
}